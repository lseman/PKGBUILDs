From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: guowangy <wangyang.guo@intel.com>
Date: Thu, 5 Dec 2019 15:16:42 +0800
Subject: [PATCH] add numpy-benchmarks for pgo

---
 Tools/numpy-benchmarks/README.rst             |  56 ++
 Tools/numpy-benchmarks/benchall.py            |  66 +++
 .../benchmarks/arc_distance.py                |  14 +
 .../numpy-benchmarks/benchmarks/check_mask.py |  14 +
 .../benchmarks/create_grid.py                 |  12 +
 Tools/numpy-benchmarks/benchmarks/cronbach.py |   9 +
 .../numpy-benchmarks/benchmarks/diffusion.py  |  19 +
 .../benchmarks/euclidean_distance_square.py   |   7 +
 Tools/numpy-benchmarks/benchmarks/evolve.py   |  11 +
 .../numpy-benchmarks/benchmarks/grayscott.py  |  31 +
 Tools/numpy-benchmarks/benchmarks/grouping.py |  11 +
 Tools/numpy-benchmarks/benchmarks/harris.py   |  31 +
 Tools/numpy-benchmarks/benchmarks/hasting.py  |  12 +
 Tools/numpy-benchmarks/benchmarks/l1norm.py   |   8 +
 Tools/numpy-benchmarks/benchmarks/l2norm.py   |   8 +
 .../numpy-benchmarks/benchmarks/laplacien.py  |  13 +
 .../benchmarks/local_maxima.py                |  27 +
 .../benchmarks/log_likelihood.py              |  11 +
 Tools/numpy-benchmarks/benchmarks/lstsqr.py   |  17 +
 .../benchmarks/make_decision.py               |  13 +
 .../benchmarks/multiple_sum.py                |  16 +
 .../benchmarks/normalize_complex_arr.py       |  10 +
 Tools/numpy-benchmarks/benchmarks/pairwise.py |   9 +
 .../benchmarks/periodic_dist.py               |  36 ++
 .../numpy-benchmarks/benchmarks/repeating.py  |  13 +
 .../benchmarks/reverse_cumsum.py              |   7 +
 Tools/numpy-benchmarks/benchmarks/rosen.py    |  10 +
 .../numpy-benchmarks/benchmarks/slowparts.py  |  17 +
 .../benchmarks/specialconvolve.py             |  10 +
 .../benchmarks/vibr_energy.py                 |   8 +
 Tools/numpy-benchmarks/benchmarks/wave.py     |  54 ++
 Tools/numpy-benchmarks/benchmarks/wdist.py    |  18 +
 Tools/numpy-benchmarks/np-bench               | 544 ++++++++++++++++++
 33 files changed, 1142 insertions(+)
 create mode 100644 Tools/numpy-benchmarks/README.rst
 create mode 100644 Tools/numpy-benchmarks/benchall.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/arc_distance.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/check_mask.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/create_grid.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/cronbach.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/diffusion.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/euclidean_distance_square.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/evolve.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/grayscott.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/grouping.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/harris.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/hasting.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/l1norm.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/l2norm.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/laplacien.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/local_maxima.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/log_likelihood.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/lstsqr.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/make_decision.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/multiple_sum.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/normalize_complex_arr.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/pairwise.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/periodic_dist.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/repeating.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/reverse_cumsum.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/rosen.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/slowparts.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/specialconvolve.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/vibr_energy.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/wave.py
 create mode 100644 Tools/numpy-benchmarks/benchmarks/wdist.py
 create mode 100755 Tools/numpy-benchmarks/np-bench

diff --git a/Tools/numpy-benchmarks/README.rst b/Tools/numpy-benchmarks/README.rst
new file mode 100644
index 0000000..7506731
--- /dev/null
+++ b/Tools/numpy-benchmarks/README.rst
@@ -0,0 +1,56 @@
+================
+Numpy Benchmarks
+================
+
+A collection of scientific kernels that use the numpy package, for benchmarking
+purpose.
+
+Usage
+=====
+
+First setup the benchmarking environment::
+
+    > ./np-bench setup
+
+Then run the whole benchmark suite::
+
+    > ./np-bench run
+
+To run a specific set of benchmarks on a specific set of compilers, use the
+ad hoc arguments , as in::
+
+    > ./np-bench run -tnumba -tpythran benchmarks/harris.py benchmarks/evolve.py
+
+It is possible to post-process the raw output of ``./np-bench run``, for
+instance to plot the result as ``png``::
+
+    > ./np-bench run -tpython -tpythran > run.log
+    > ./np-bench format  -tpng run.log
+
+Kernels
+=======
+
+Each kernel holds a ``#setup: ... code ...`` and a ``#run: ... code ...``
+comment line to be passed to the ``timeit`` module for easy benchmarking, as
+automated by the ``np-bench`` script.
+
+Each kernel involve some high-level numpy construct, sometimes mixed with
+explicit iteration.
+
+Example
+=======
+
+Let's analyze some output::
+
+    > ./np-bench run -tpython benchmarks/harris.py
+    harris Python 5431 5454 14
+
+What does it mean? The code from ``benchmarks/harris.py`` was run through
+``timeit`` using the ``#setup`` and ``#run`` code. It outputs (in that order
+and in nanoseconds):
+
+1. the **best** execution time among all runs;
+2. the **average** execution time of the runs;
+3. the **standard deviation** of the runs.
+
+:x
diff --git a/Tools/numpy-benchmarks/benchall.py b/Tools/numpy-benchmarks/benchall.py
new file mode 100644
index 0000000..44b2520
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchall.py
@@ -0,0 +1,66 @@
+import os
+import re
+import sys
+from timeit import Timer, default_timer
+import numpy
+
+
+ROOT_DIR = os.path.abspath(os.path.dirname(__file__))
+BENCHMARKS_DIR = os.path.join(ROOT_DIR, 'benchmarks')
+sys.path.insert(0, BENCHMARKS_DIR)
+
+
+def extractor(fpath):
+    '''
+    extract #setup and #run segment
+    '''
+    re_setup = re.compile('^#setup: (.*)$')
+    re_run = re.compile('^#run: (.*)$')
+    with open(fpath) as fp:
+        for line in fp.readlines():
+            m = re_setup.match(line)
+            if m:
+                setup = m.group(1)
+            m = re_run.match(line)
+            if m:
+                run = 'res = ' + m.group(1)
+        try:
+            return setup, run
+        except NameError:
+            raise RuntimeError('%s has invalid header' % fpath)
+
+
+def code_gen(fpath):
+    basename = os.path.basename(fpath)
+    function, _ = os.path.splitext(basename)
+    setup, run = extractor(fpath)
+    setup_code = ';'.join([
+        setup,
+        'from {module} import {func}'.format(module=function, func=function),
+    ])
+    return function, setup_code, run
+
+
+def do_bench(fpath, repeat=3, number=40):
+    name, setup, run = code_gen(fpath)
+    print(name, end=" ", flush=True)
+
+    t = Timer(run, setup, default_timer)
+    try:
+        r = t.repeat(repeat, number)
+    except:  # NOQA
+        t.print_exc()
+        return 1
+    r = [int(x * 1e6 / number) for x in r]
+    best = min(r)
+    average = int(numpy.average(r))
+    std = int(numpy.std(r))
+    print(best, average, std, flush=True)
+
+
+if __name__ == '__main__':
+    import glob
+
+    print('name\tbest\taverage\tstd', flush=True)
+    for b in glob.glob(BENCHMARKS_DIR + '/*.py'):
+        do_bench(b)
diff --git a/Tools/numpy-benchmarks/benchmarks/arc_distance.py b/Tools/numpy-benchmarks/benchmarks/arc_distance.py
new file mode 100644
index 0000000..f568640
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/arc_distance.py
@@ -0,0 +1,14 @@
+#setup: N = 10000 ; import numpy as np ; np.random.seed(0); t0, p0, t1, p1 = np.random.randn(N), np.random.randn(N), np.random.randn(N), np.random.randn(N)
+#run: arc_distance(t0, p0, t1, p1)
+
+#pythran export arc_distance(float64 [], float64[], float64[], float64[])
+
+import numpy as np
+def arc_distance(theta_1, phi_1,
+                       theta_2, phi_2):
+    """
+    Calculates the pairwise arc distance between all points in vector a and b.
+    """
+    temp = np.sin((theta_2-theta_1)/2)**2+np.cos(theta_1)*np.cos(theta_2)*np.sin((phi_2-phi_1)/2)**2
+    distance_matrix = 2 * (np.arctan2(np.sqrt(temp),np.sqrt(1-temp)))
+    return distance_matrix
diff --git a/Tools/numpy-benchmarks/benchmarks/check_mask.py b/Tools/numpy-benchmarks/benchmarks/check_mask.py
new file mode 100644
index 0000000..9ed6f1f
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/check_mask.py
@@ -0,0 +1,14 @@
+#setup: n=1000 ; import numpy as np; np.random.seed(0); db = np.array(np.random.randint(2, size=(n, 4)), dtype=bool)
+#run: check_mask(db)
+#from: http://stackoverflow.com/questions/34500913/numba-slower-for-numpy-bitwise-and-on-boolean-arrays
+
+#pythran export check_mask(bool[][])
+import numpy as np
+def check_mask(db, mask=[1, 0, 1]):
+    out = np.zeros(db.shape[0],dtype=bool)
+    for idx, line in enumerate(db):
+        target, vector = line[0], line[1:]
+        if (mask == np.bitwise_and(mask, vector)).all():
+            if target == 1:
+                out[idx] = 1
+    return out
diff --git a/Tools/numpy-benchmarks/benchmarks/create_grid.py b/Tools/numpy-benchmarks/benchmarks/create_grid.py
new file mode 100644
index 0000000..98b9314
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/create_grid.py
@@ -0,0 +1,12 @@
+#from: http://stackoverflow.com/questions/13815719/creating-grid-with-numpy-performance
+#pythran export create_grid(float [])
+#setup: import numpy as np ; N = 800 ; x = np.arange(0,1,1./N)
+#run: create_grid(x)
+import numpy as np
+def create_grid(x):
+    N = x.shape[0]
+    z = np.zeros((N, N, 3))
+    z[:,:,0] = x.reshape(-1,1)
+    z[:,:,1] = x
+    fast_grid = z.reshape(N*N, 3)
+    return fast_grid
diff --git a/Tools/numpy-benchmarks/benchmarks/cronbach.py b/Tools/numpy-benchmarks/benchmarks/cronbach.py
new file mode 100644
index 0000000..b300068
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/cronbach.py
@@ -0,0 +1,9 @@
+#from: http://stackoverflow.com/questions/20799403/improving-performance-of-cronbach-alpha-code-python-numpy
+#pythran export cronbach(float [][])
+#setup: import numpy as np ; np.random.seed(0); N = 600 ; items = np.random.rand(N,N)
+#run: cronbach(items)
+def cronbach(itemscores):
+    itemvars = itemscores.var(axis=1, ddof=1)
+    tscores = itemscores.sum(axis=0)
+    nitems = len(itemscores)
+    return nitems / (nitems-1) * (1 - itemvars.sum() / tscores.var(ddof=1))
diff --git a/Tools/numpy-benchmarks/benchmarks/diffusion.py b/Tools/numpy-benchmarks/benchmarks/diffusion.py
new file mode 100644
index 0000000..07845b4
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/diffusion.py
@@ -0,0 +1,19 @@
+#setup: import numpy as np;lx,ly=(2**7,2**7);u=np.zeros([lx,ly],dtype=np.double);u[lx//2,ly//2]=1000.0;tempU=np.zeros([lx,ly],dtype=np.double)
+#run: diffusion(u,tempU,100)
+
+#pythran export diffusion(float [][], float [][], int)
+import numpy as np
+
+
+def diffusion(u, tempU, iterNum):
+    """
+    Apply Numpy matrix for the Forward-Euler Approximation
+    """
+    mu = .1
+
+    for n in range(iterNum):
+        tempU[1:-1, 1:-1] = u[1:-1, 1:-1] + mu * (
+            u[2:, 1:-1] - 2 * u[1:-1, 1:-1] + u[0:-2, 1:-1] +
+            u[1:-1, 2:] - 2 * u[1:-1, 1:-1] + u[1:-1, 0:-2])
+        u[:, :] = tempU[:, :]
+        tempU[:, :] = 0.0
diff --git a/Tools/numpy-benchmarks/benchmarks/euclidean_distance_square.py b/Tools/numpy-benchmarks/benchmarks/euclidean_distance_square.py
new file mode 100644
index 0000000..12eaae7
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/euclidean_distance_square.py
@@ -0,0 +1,7 @@
+#from:  https://stackoverflow.com/questions/50658884/why-this-numba-code-is-6x-slower-than-numpy-code
+#setup: import numpy as np; np.random.seed(0); x1 = np.random.random((1, 512)); x2 = np.random.random((10000, 512))
+#run: euclidean_distance_square(x1, x2)
+#pythran export euclidean_distance_square(float64[1,:], float64[:,:])
+import numpy as np
+def euclidean_distance_square(x1, x2):
+    return -2*np.dot(x1, x2.T) + np.sum(np.square(x1), axis=1)[:, np.newaxis] + np.sum(np.square(x2), axis=1)
diff --git a/Tools/numpy-benchmarks/benchmarks/evolve.py b/Tools/numpy-benchmarks/benchmarks/evolve.py
new file mode 100644
index 0000000..79a8fcd
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/evolve.py
@@ -0,0 +1,11 @@
+#setup: import numpy as np ; grid_shape = (512, 512) ; grid = np.zeros(grid_shape) ; block_low = int(grid_shape[0] * .4) ; block_high = int(grid_shape[0] * .5) ; grid[block_low:block_high, block_low:block_high] = 0.005
+#run: evolve(grid, 0.1)
+#from: High Performance Python by Micha Gorelick and Ian Ozsvald, http://shop.oreilly.com/product/0636920028963.do
+
+#pythran export evolve(float64[][], float)
+import numpy as np
+def laplacian(grid):
+    return np.roll(grid, +1, 0) + np.roll(grid, -1, 0) + np.roll(grid, +1, 1) + np.roll(grid, -1, 1) - 4 * grid
+
+def evolve(grid, dt, D=1):
+    return grid + dt * D * laplacian(grid)
diff --git a/Tools/numpy-benchmarks/benchmarks/grayscott.py b/Tools/numpy-benchmarks/benchmarks/grayscott.py
new file mode 100644
index 0000000..1fb6b2a
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/grayscott.py
@@ -0,0 +1,31 @@
+#from http://stackoverflow.com/questions/26823312/numba-or-cython-acceleration-in-reaction-diffusion-algorithm
+#setup: pass
+#run: grayscott(40, 0.16, 0.08, 0.04, 0.06)
+
+#pythran export grayscott(int, float, float, float, float)
+import numpy as np
+def grayscott(counts, Du, Dv, F, k):
+    n = 300
+    U = np.zeros((n+2,n+2), dtype=np.float32)
+    V = np.zeros((n+2,n+2), dtype=np.float32)
+    u, v = U[1:-1,1:-1], V[1:-1,1:-1]
+
+    r = 20
+    u[:] = 1.0
+    U[n//2-r:n//2+r,n//2-r:n//2+r] = 0.50
+    V[n//2-r:n//2+r,n//2-r:n//2+r] = 0.25
+    u += 0.15*np.random.random((n,n))
+    v += 0.15*np.random.random((n,n))
+
+    for i in range(counts):
+        Lu = (                 U[0:-2,1:-1] +
+              U[1:-1,0:-2] - 4*U[1:-1,1:-1] + U[1:-1,2:] +
+                               U[2:  ,1:-1] )
+        Lv = (                 V[0:-2,1:-1] +
+              V[1:-1,0:-2] - 4*V[1:-1,1:-1] + V[1:-1,2:] +
+                               V[2:  ,1:-1] )
+        uvv = u*v*v
+        u += Du*Lu - uvv + F*(1 - u)
+        v += Dv*Lv + uvv - (F + k)*v
+
+    return V
diff --git a/Tools/numpy-benchmarks/benchmarks/grouping.py b/Tools/numpy-benchmarks/benchmarks/grouping.py
new file mode 100644
index 0000000..df48cab
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/grouping.py
@@ -0,0 +1,11 @@
+#from: http://stackoverflow.com/questions/4651683/numpy-grouping-using-itertools-groupby-performance
+#setup: import numpy as np ; N = 500000 ; np.random.seed(0); values = np.array(np.random.randint(0,3298,size=N),dtype='u4') ; values.sort()
+#run: grouping(values)
+
+#pythran export grouping(uint32 [])
+
+def grouping(values):
+    import numpy as np
+    diff = np.concatenate(([1], np.diff(values)))
+    idx = np.concatenate((np.where(diff)[0], [len(values)]))
+    return values[idx[:-1]], np.diff(idx)
diff --git a/Tools/numpy-benchmarks/benchmarks/harris.py b/Tools/numpy-benchmarks/benchmarks/harris.py
new file mode 100644
index 0000000..ff50913
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/harris.py
@@ -0,0 +1,31 @@
+#from: parakeet testbed
+#setup: import numpy as np ; M, N = 512, 512 ; I = np.random.randn(M,N)
+#run: harris(I)
+
+#pythran export harris(float64[][])
+import numpy as np
+
+
+
+def harris(I):
+  m,n = I.shape
+  dx = (I[1:, :] - I[:m-1, :])[:, 1:]
+  dy = (I[:, 1:] - I[:, :n-1])[1:, :]
+
+  #
+  #   At each point we build a matrix
+  #   of derivative products
+  #   M =
+  #   | A = dx^2     C = dx * dy |
+  #   | C = dy * dx  B = dy * dy |
+  #
+  #   and the score at that point is:
+  #      det(M) - k*trace(M)^2
+  #
+  A = dx * dx
+  B = dy * dy
+  C = dx * dy
+  tr = A + B
+  det = A * B - C * C
+  k = 0.05
+  return det - k * tr * tr
diff --git a/Tools/numpy-benchmarks/benchmarks/hasting.py b/Tools/numpy-benchmarks/benchmarks/hasting.py
new file mode 100644
index 0000000..02f8612
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/hasting.py
@@ -0,0 +1,12 @@
+#from: http://wiki.scipy.org/Cookbook/Theoretical_Ecology/Hastings_and_Powell
+#setup: import numpy as np ; y = np.random.rand(3) ; args = np.random.rand(7)
+#run: hasting(y, *args)
+
+#pythran export hasting(float [], float, float, float, float, float, float, float)
+import numpy as np
+def hasting(y, t, a1, a2, b1, b2, d1, d2):
+    yprime = np.empty((3,))
+    yprime[0] = y[0] * (1. - y[0]) - a1*y[0]*y[1]/(1. + b1 * y[0])
+    yprime[1] = a1*y[0]*y[1] / (1. + b1 * y[0]) - a2 * y[1]*y[2] / (1. + b2 * y[1]) - d1 * y[1]
+    yprime[2] = a2*y[1]*y[2]/(1. + b2*y[1]) - d2*y[2]
+    return yprime
diff --git a/Tools/numpy-benchmarks/benchmarks/l1norm.py b/Tools/numpy-benchmarks/benchmarks/l1norm.py
new file mode 100644
index 0000000..530c9da
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/l1norm.py
@@ -0,0 +1,8 @@
+#from: https://stackoverflow.com/questions/55854611/efficient-way-of-vectorizing-distance-calculation/55877642#55877642
+#setup: import numpy as np ; N = 80; x = np.random.rand(N,N); y = np.random.rand(N,N)
+#run: l1norm(x, y)
+
+#pythran export l1norm(float64[][], float64[:,:])
+import numpy as np
+def l1norm(x, y):
+    return np.sum(np.abs(x[:, None, :] - y), axis=-1)
diff --git a/Tools/numpy-benchmarks/benchmarks/l2norm.py b/Tools/numpy-benchmarks/benchmarks/l2norm.py
new file mode 100644
index 0000000..5eb610f
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/l2norm.py
@@ -0,0 +1,8 @@
+#from: http://stackoverflow.com/questions/7741878/how-to-apply-numpy-linalg-norm-to-each-row-of-a-matrix/7741976#7741976
+#setup: import numpy as np ; N = 1000; x = np.random.rand(N,N)
+#run: l2norm(x)
+
+#pythran export l2norm(float64[][])
+import numpy as np
+def l2norm(x):
+    return np.sqrt(np.sum(np.abs(x)**2, 1))
diff --git a/Tools/numpy-benchmarks/benchmarks/laplacien.py b/Tools/numpy-benchmarks/benchmarks/laplacien.py
new file mode 100644
index 0000000..1094d3e
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/laplacien.py
@@ -0,0 +1,13 @@
+#setup: import numpy as np ; N = 500 ; X = np.random.randn(N,N,3)
+#run: laplacien(X)
+#pythran export laplacien(float64[][][3])
+
+import numpy as np
+def laplacien(image):
+        out_image = np.abs(4*image[1:-1,1:-1] -
+                                       image[0:-2,1:-1] - image[2:,1:-1] -
+                                       image[1:-1,0:-2] - image[1:-1,2:])
+        valmax = np.max(out_image)
+        valmax = max(1.,valmax)+1.E-9
+        out_image /= valmax
+        return out_image
diff --git a/Tools/numpy-benchmarks/benchmarks/local_maxima.py b/Tools/numpy-benchmarks/benchmarks/local_maxima.py
new file mode 100644
index 0000000..04c0591
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/local_maxima.py
@@ -0,0 +1,27 @@
+#from: https://github.com/iskandr/parakeet/blob/master/benchmarks/nd_local_maxima.py
+#setup: import numpy as np ; shape = (5,4,3,2) ; x = np.arange(120, dtype=np.float64).reshape(*shape)
+#run: local_maxima(x)
+
+#pythran export local_maxima(float [][][][])
+import numpy as np
+
+def wrap(pos, offset, bound):
+    return ( pos + offset ) % bound
+
+def clamp(pos, offset, bound):
+    return min(bound-1,max(0,pos+offset))
+
+def reflect(pos, offset, bound):
+    idx = pos+offset
+    return min(2*(bound-1)-idx,max(idx,-idx))
+
+
+def local_maxima(data, mode=wrap):
+  wsize = data.shape
+  result = np.ones(data.shape, bool)
+  for pos in np.ndindex(data.shape):
+    myval = data[pos]
+    for offset in np.ndindex(wsize):
+      neighbor_idx = tuple(mode(p, o-w//2, w) for (p, o, w) in zip(pos, offset, wsize))
+      result[pos] &= (data[neighbor_idx] <= myval)
+  return result
diff --git a/Tools/numpy-benchmarks/benchmarks/log_likelihood.py b/Tools/numpy-benchmarks/benchmarks/log_likelihood.py
new file mode 100644
index 0000000..848d453
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/log_likelihood.py
@@ -0,0 +1,11 @@
+#setup: import numpy as np ; N = 100000 ; a = np.random.random(N); b = 0.1; c =1.1
+#run: log_likelihood(a, b, c)
+#from: http://arogozhnikov.github.io/2015/09/08/SpeedBenchmarks.html
+import numpy
+
+#pythran export log_likelihood(float64[], float64, float64)
+def log_likelihood(data, mean, sigma):
+    s = (data - mean) ** 2 / (2 * (sigma ** 2))
+    pdfs = numpy.exp(- s)
+    pdfs /= numpy.sqrt(2 * numpy.pi) * sigma
+    return numpy.log(pdfs).sum()
diff --git a/Tools/numpy-benchmarks/benchmarks/lstsqr.py b/Tools/numpy-benchmarks/benchmarks/lstsqr.py
new file mode 100644
index 0000000..10f2b83
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/lstsqr.py
@@ -0,0 +1,17 @@
+#setup: import numpy as np ; N = 500000 ; X, Y = np.random.rand(N), np.random.rand(N)
+#run: lstsqr(X, Y)
+#from: http://nbviewer.ipython.org/github/rasbt/One-Python-benchmark-per-day/blob/master/ipython_nbs/day10_fortran_lstsqr.ipynb
+
+#pythran export lstsqr(float[], float[])
+import numpy as np
+def lstsqr(x, y):
+    """ Computes the least-squares solution to a linear matrix equation. """
+    x_avg = np.average(x)
+    y_avg = np.average(y)
+    dx = x - x_avg
+    dy = y - y_avg
+    var_x = np.sum(dx**2)
+    cov_xy = np.sum(dx * (y - y_avg))
+    slope = cov_xy / var_x
+    y_interc = y_avg - slope*x_avg
+    return (slope, y_interc)
diff --git a/Tools/numpy-benchmarks/benchmarks/make_decision.py b/Tools/numpy-benchmarks/benchmarks/make_decision.py
new file mode 100644
index 0000000..742b5c8
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/make_decision.py
@@ -0,0 +1,13 @@
+#setup: import numpy as np, random; np.random.seed(0); s=np.random.randn(2**16)+np.random.randn(2**16)*1.j ; sc=np.random.choice(s, 32)
+#run: make_decision(s, sc)
+#from: https://github.com/serge-sans-paille/pythran/issues/801
+
+import numpy as np
+#pythran export make_decision(complex128[], complex128[])
+def make_decision(E, symbols):
+    L = E.shape[0]
+    syms_out = np.zeros(L, dtype=E.dtype)
+    for i in range(L):
+        im = np.argmin(abs(E[i]-symbols)**2)
+        syms_out[i] = symbols[im]
+    return syms_out
diff --git a/Tools/numpy-benchmarks/benchmarks/multiple_sum.py b/Tools/numpy-benchmarks/benchmarks/multiple_sum.py
new file mode 100644
index 0000000..580c9ad
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/multiple_sum.py
@@ -0,0 +1,16 @@
+#from http://stackoverflow.com/questions/77999777799977/numpy-vs-cython-speed
+#pythran export multiple_sum(float[][])
+#setup: import numpy as np ; r = np.random.rand(100,100)
+#run: multiple_sum(r)
+import numpy as np
+def multiple_sum(array):
+
+    rows = array.shape[0]
+    cols = array.shape[1]
+
+    out = np.zeros((rows, cols))
+
+    for row in range(0, rows):
+        out[row, :] = np.sum(array - array[row, :], 0)
+
+    return out
diff --git a/Tools/numpy-benchmarks/benchmarks/normalize_complex_arr.py b/Tools/numpy-benchmarks/benchmarks/normalize_complex_arr.py
new file mode 100644
index 0000000..93d2b99
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/normalize_complex_arr.py
@@ -0,0 +1,10 @@
+import numpy as np
+#from: https://stackoverflow.com/questions/41576536/normalizing-complex-values-in-numpy-python
+#setup: import numpy as np; np.random.seed(0); N = 10000; x = np.random.random(N) + 1j *  np.random.random(N)
+#run: normalize_complex_arr(x)
+
+#pythran export normalize_complex_arr(complex[])
+
+def normalize_complex_arr(a):
+    a_oo = a - a.real.min() - 1j*a.imag.min() # origin offsetted
+    return a_oo/np.abs(a_oo).max()
diff --git a/Tools/numpy-benchmarks/benchmarks/pairwise.py b/Tools/numpy-benchmarks/benchmarks/pairwise.py
new file mode 100644
index 0000000..4c8695a
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/pairwise.py
@@ -0,0 +1,9 @@
+#from: http://people.duke.edu/~ccc14/sta-663-2016/03A_Numbers.html#Example:-Calculating-pairwise-distance-matrix-using-broadcasting-and-vectorization
+#setup: import numpy as np ; X = np.linspace(0,10,20000).reshape(200,100)
+#run: pairwise(X)
+
+#pythran export pairwise(float [][])
+
+import numpy as np
+def pairwise(pts):
+    return np.sum((pts[None,:] - pts[:, None])**2, -1)**0.5
diff --git a/Tools/numpy-benchmarks/benchmarks/periodic_dist.py b/Tools/numpy-benchmarks/benchmarks/periodic_dist.py
new file mode 100644
index 0000000..39d30e6
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/periodic_dist.py
@@ -0,0 +1,36 @@
+#setup: import numpy as np ; N = 20 ; x = y = z = np.arange(0., N, 0.1) ; L = 4 ; periodic = True
+#run: periodic_dist(x, x, x, L,periodic, periodic, periodic)
+
+#pythran export periodic_dist(float [], float[], float[], int, bool, bool, bool)
+import numpy as np
+
+def periodic_dist(x, y, z, L, periodicX, periodicY, periodicZ):
+    " ""Computes distances between all particles and places the result in a matrix such that the ij th matrix entry corresponds to the distance between particle i and j"" "
+    N = len(x)
+    xtemp = np.tile(x,(N,1))
+    dx = xtemp - xtemp.T
+    ytemp = np.tile(y,(N,1))
+    dy = ytemp - ytemp.T
+    ztemp = np.tile(z,(N,1))
+    dz = ztemp - ztemp.T
+
+    # Particles 'feel' each other across the periodic boundaries
+    if periodicX:
+        dx[dx>L/2]=dx[dx > L/2]-L
+        dx[dx<-L/2]=dx[dx < -L/2]+L
+
+    if periodicY:
+        dy[dy>L/2]=dy[dy>L/2]-L
+        dy[dy<-L/2]=dy[dy<-L/2]+L
+
+    if periodicZ:
+        dz[dz>L/2]=dz[dz>L/2]-L
+        dz[dz<-L/2]=dz[dz<-L/2]+L
+
+    # Total Distances
+    d = np.sqrt(dx**2+dy**2+dz**2)
+
+    # Mark zero entries with negative 1 to avoid divergences
+    d[d==0] = -1
+
+    return d, dx, dy, dz
diff --git a/Tools/numpy-benchmarks/benchmarks/repeating.py b/Tools/numpy-benchmarks/benchmarks/repeating.py
new file mode 100644
index 0000000..1ca0dd4
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/repeating.py
@@ -0,0 +1,13 @@
+#from: http://stackoverflow.com/questions/14553331/how-to-improve-numpy-performance-in-this-short-code
+#pythran export repeating(float[], int)
+#setup: import numpy as np ; a = np.random.rand(10000)
+#run: repeating(a, 20)
+
+import numpy as np
+
+def repeating(x, nvar_y):
+    nvar_x = x.shape[0]
+    y = np.empty(nvar_x*(1+nvar_y))
+    y[0:nvar_x] = x[0:nvar_x]
+    y[nvar_x:] = np.repeat(x,nvar_y)
+    return y
diff --git a/Tools/numpy-benchmarks/benchmarks/reverse_cumsum.py b/Tools/numpy-benchmarks/benchmarks/reverse_cumsum.py
new file mode 100644
index 0000000..2aedefd
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/reverse_cumsum.py
@@ -0,0 +1,7 @@
+#from: http://stackoverflow.com/questions/16541618/perform-a-reverse-cumulative-sum-on-a-numpy-array
+#pythran export reverse_cumsum(float[])
+#setup: import numpy as np ; r = np.random.rand(1000000)
+#run: reverse_cumsum(r)
+import numpy as np
+def reverse_cumsum(x):
+    return np.cumsum(x[::-1])[::-1]
diff --git a/Tools/numpy-benchmarks/benchmarks/rosen.py b/Tools/numpy-benchmarks/benchmarks/rosen.py
new file mode 100644
index 0000000..0e73b70
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/rosen.py
@@ -0,0 +1,10 @@
+#setup: import numpy as np; r = np.arange(1000000, dtype=float)
+#run: rosen(r)
+import numpy as np
+
+#pythran export rosen(float[])
+
+def rosen(x):
+    t0 = 100 * (x[1:] - x[:-1] ** 2) ** 2
+    t1 = (1 - x[:-1]) ** 2
+    return np.sum(t0 + t1)
diff --git a/Tools/numpy-benchmarks/benchmarks/slowparts.py b/Tools/numpy-benchmarks/benchmarks/slowparts.py
new file mode 100644
index 0000000..39bee5a
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/slowparts.py
@@ -0,0 +1,17 @@
+#from: https://groups.google.com/forum/#!topic/parakeet-python/p-flp2kdE4U
+#setup: import numpy as np ;d = 10 ;re = 5 ;params = (d, re, np.ones((2*d, d+1, re)), np.ones((d, d+1, re)),  np.ones((d, 2*d)), np.ones((d, 2*d)), np.ones((d+1, re, d)), np.ones((d+1, re, d)), 1)
+#run: slowparts(*params)
+
+#pythran export slowparts(int, int, float [][][], float [][][], float [][], float [][], float [][][], float [][][], int)
+from numpy import zeros, power, tanh
+def slowparts(d, re, preDz, preWz, SRW, RSW, yxV, xyU, resid):
+    """ computes the linear algebra intensive part of the gradients of the grae
+    """
+    fprime = lambda x: 1 - power(tanh(x), 2)
+
+    partialDU = zeros((d+1, re, 2*d, d))
+    for k in range(2*d):
+        for i in range(d):
+            partialDU[:,:,k,i] = fprime(preDz[k]) * fprime(preWz[i]) * (SRW[i,k] + RSW[i,k]) * yxV[:,:,i]
+
+    return partialDU
diff --git a/Tools/numpy-benchmarks/benchmarks/specialconvolve.py b/Tools/numpy-benchmarks/benchmarks/specialconvolve.py
new file mode 100644
index 0000000..7b75a13
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/specialconvolve.py
@@ -0,0 +1,10 @@
+#from: http://stackoverflow.com/questions/2196693/improving-numpy-performance
+#pythran export specialconvolve(uint32 [][])
+#setup: import numpy as np ; r = np.arange(100*10000, dtype=np.uint32).reshape(1000,1000)
+#run: specialconvolve(r)
+
+def specialconvolve(a):
+    # sorry, you must pad the input yourself
+    rowconvol = a[1:-1,:] + a[:-2,:] + a[2:,:]
+    colconvol = rowconvol[:,1:-1] + rowconvol[:,:-2] + rowconvol[:,2:] - 9*a[1:-1,1:-1]
+    return colconvol
diff --git a/Tools/numpy-benchmarks/benchmarks/vibr_energy.py b/Tools/numpy-benchmarks/benchmarks/vibr_energy.py
new file mode 100644
index 0000000..1ffa489
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/vibr_energy.py
@@ -0,0 +1,8 @@
+#from: http://stackoverflow.com/questions/17112550/python-and-numba-for-vectorized-functions
+#setup: import numpy as np ; N = 100000 ; a, b, c = np.random.rand(N), np.random.rand(N), np.random.rand(N)
+#run: vibr_energy(a, b, c)
+
+#pythran export vibr_energy(float64[], float64[], float64[])
+import numpy
+def vibr_energy(harmonic, anharmonic, i):
+    return numpy.exp(-harmonic * i - anharmonic * (i ** 2))
diff --git a/Tools/numpy-benchmarks/benchmarks/wave.py b/Tools/numpy-benchmarks/benchmarks/wave.py
new file mode 100644
index 0000000..c126911
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/wave.py
@@ -0,0 +1,54 @@
+#from https://github.com/sklam/numba-example-wavephysics
+#setup: N=4000
+#run: wave(N)
+import numpy as np
+from math import ceil
+
+def physics(masspoints, dt, plunk, which):
+  ppos = masspoints[1]
+  cpos = masspoints[0]
+  N = cpos.shape[0]
+  # apply hooke's law
+  HOOKE_K = 2100000.
+  DAMPING = 0.0001
+  MASS = .01
+
+  force = np.zeros((N, 2))
+  for i in range(1, N):
+    dx, dy = cpos[i] - cpos[i - 1]
+    dist = np.sqrt(dx**2 + dy**2)
+    assert dist != 0
+    fmag = -HOOKE_K * dist
+    cosine = dx / dist
+    sine = dy / dist
+    fvec = np.array([fmag * cosine, fmag * sine])
+    force[i - 1] -= fvec
+    force[i] += fvec
+
+  force[0] = force[-1] = 0, 0
+  force[which][1] += plunk
+  accel = force / MASS
+
+  # verlet integration
+  npos = (2 - DAMPING) * cpos - (1 - DAMPING) * ppos + accel * (dt**2)
+
+  masspoints[1] = cpos
+  masspoints[0] = npos
+
+#pythran export wave(int)
+def wave(PARTICLE_COUNT):
+    SUBDIVISION = 300
+    FRAMERATE = 60
+    count = PARTICLE_COUNT
+    width, height = 1200, 400
+
+    masspoints = np.empty((2, count, 2), np.float64)
+    initpos = np.zeros(count, np.float64)
+    for i in range(1, count):
+        initpos[i] = initpos[i - 1] + float(width) / count
+    masspoints[:, :, 0] = initpos
+    masspoints[:, :, 1] = height / 2
+    f = 15
+    plunk_pos = count // 2
+    physics( masspoints, 1./ (SUBDIVISION * FRAMERATE), f, plunk_pos)
+    return masspoints[0, count // 2]
diff --git a/Tools/numpy-benchmarks/benchmarks/wdist.py b/Tools/numpy-benchmarks/benchmarks/wdist.py
new file mode 100644
index 0000000..c19c4a7
--- /dev/null
+++ b/Tools/numpy-benchmarks/benchmarks/wdist.py
@@ -0,0 +1,18 @@
+#from: http://stackoverflow.com/questions/19277244/fast-weighted-euclidean-distance-between-points-in-arrays/19277334#19277334
+#setup: import numpy as np ; N = 70 ; A = np.random.rand(N,N) ; B =  np.random.rand(N,N) ; W = np.random.rand(N,N)
+#run: wdist(A,B,W)
+
+#pythran export wdist(float64 [][], float64 [][], float64[][])
+
+import numpy as np
+def wdist(A, B, W):
+
+    k,m = A.shape
+    _,n = B.shape
+    D = np.zeros((m, n))
+
+    for ii in range(m):
+        for jj in range(n):
+            wdiff = (A[:,ii] - B[:,jj]) / W[:,ii]
+            D[ii,jj] = np.sqrt((wdiff**2).sum())
+    return D
diff --git a/Tools/numpy-benchmarks/np-bench b/Tools/numpy-benchmarks/np-bench
new file mode 100755
index 0000000..1ed6530
--- /dev/null
+++ b/Tools/numpy-benchmarks/np-bench
@@ -0,0 +1,544 @@
+#! /usr/bin/env python
+import re
+import tempfile
+import os.path
+import random
+import stat
+import glob
+import shutil
+import venv
+import subprocess
+
+###############################################################################
+# Run command
+
+class PythonExtractor(object):
+    '''
+    Generate test case for Python from run and setup comment in a benchmark
+    '''
+
+    name = 'Python'
+
+    def __init__(self, output_dir):
+        self.re_setup = re.compile('^#setup: (.*)$')
+        self.re_run = re.compile('^#run: (.*)$')
+        self.output_dir = output_dir
+
+    def process_lines(self, filename, lines):
+        content = []
+        for line in lines:
+            m = self.re_setup.match(line)
+            if m:
+                setup = m.group(1)
+            m = self.re_run.match(line)
+            if m:
+                run = 'res = ' + m.group(1)
+            content.append(line)
+        try:
+            return setup, run, content
+        except NameError as n:
+            raise RuntimeError('%s has invalid header' % filename)
+
+    def __call__(self, filename):
+        with open(filename) as fd:
+            s, r, c = self.process_lines(filename, fd)
+            return s, r, ''.join(c)
+
+    def compile(self, filename):
+        pass
+
+
+class PythranExtractor(PythonExtractor):
+    '''
+    Generate test case for Pythran from run and setup comment in a benchmark
+
+    Handle the conversion from python to shared library from #pythran comment.
+    '''
+
+    name = 'pythran'
+
+    def compile(self, filename):
+        import pythran, os
+        cwd = os.getcwd()
+        os.chdir(self.output_dir)
+        try:
+            pythran.compile_pythranfile(os.path.join(cwd, filename))
+        finally:
+            os.chdir(cwd)
+
+
+class JitExtractor(PythonExtractor):
+    '''
+    Helper class for jit-based extractor
+    '''
+
+    def process_lines(self, filename, lines):
+        s, r, c = super(JitExtractor, self).process_lines(filename, lines)
+        lines = [self.extra_import]
+        for line in c:
+            if line.startswith('def '):
+                lines.append(self.decorator)
+            lines.append(line)
+        return s, r, lines
+
+
+class NumbaExtractor(JitExtractor):
+    '''
+    Generate test case for Numba from run and setup comment in a benchmark
+
+    Add a generic @jit to the main kernel.
+    '''
+
+    name = 'numba'
+
+    def __init__(self, output_dir):
+        super(NumbaExtractor, self).__init__(output_dir)
+        self.extra_import = 'import numba\n'
+        self.decorator = '@numba.jit\n'
+
+
+class HopeExtractor(JitExtractor):
+    '''
+    Generate test case for Hope from run and setup comment in a benchmark
+
+    Add a generic @jit to the main kernel.
+    '''
+
+    name = 'hope'
+
+    def __init__(self, output_dir):
+        super(HopeExtractor, self).__init__(output_dir)
+        self.extra_import = 'import hope\n'
+        self.decorator = '@hope.jit\n'
+
+
+def run_generator(basedir, filenames, extractors):
+    try:
+        os.makedirs(basedir)
+    except FileExistsError:
+        pass
+    location = tempfile.mkdtemp(prefix='rundir_', dir=basedir)
+    with open(os.path.join(location, 'benchit.py'), 'w') as dst:
+        dst.write(benchit)
+
+    shelllines = []
+    cwd = os.getcwd()
+    for extractor in extractors:
+        e = extractor(location)
+        for filename in filenames:
+            basename = os.path.basename(filename)
+            function, _ = os.path.splitext(basename)
+            tmpfilename = '_'.join([extractor.name, basename])
+            tmpmodule, _ = os.path.splitext(tmpfilename)
+            where = os.path.join(location, tmpfilename)
+            try:
+                setup, run, content = e(filename)
+                open(where, 'w').write(content)
+                e.compile(where)
+                shelllines.append(
+                    'printf "{function} {extractor} " '
+                    '&& {python} benchit.py -r 11 -n 40 -s "{setup}; from {module} import {function} ; {run}" "{run}"  '
+                    '|| echo unsupported'.format(
+                        cwd=cwd,
+                        python=sys.executable,
+                        setup=setup,
+                        module=tmpmodule,
+                        function=function,
+                        run=run,
+                        extractor=extractor.name))
+            except Exception as f:
+                shelllines.append('echo "{function} {extractor} unsupported"'.format(function=function, extractor=extractor.name))
+
+    # shuffle to hide locality effects
+    random.shuffle(shelllines)
+    shelllines = ['#!/bin/sh', 'export OMP_NUM_THREADS=1', 'cd `dirname $0`'] + shelllines
+
+    shellscript = os.path.join(location, 'run.sh')
+    open(shellscript, 'w').write('\n'.join(shelllines))
+    os.chmod(shellscript, stat.S_IXUSR | stat.S_IRUSR)
+    return shellscript
+
+
+default_targets=['python', 'pythran', 'numba', 'hope']
+
+
+def run(args):
+    if args.targets is None:
+        args.targets = default_targets
+
+    conv = lambda t: globals()[t.capitalize() + 'Extractor']
+    args.targets = [conv(t) for t in args.targets]
+
+    script = run_generator(os.path.join(args.venv, "run"), args.benchmarks, args.targets)
+    os.execl(script, script)
+
+
+###############################################################################
+# clean command
+
+def clean(args):
+
+    for d in glob.glob(os.path.join(args.venv, "run", "rundir_*")):
+        shutil.rmtree(d)
+
+
+###############################################################################
+# dump command
+
+def read_data(log, normalize='Python'):
+    averages = dict()
+    testcases = set()
+    compilers = set()
+    for line in open(log).readlines():
+        try:
+            testcase, compiler, best, average, _ = line.split()
+        except:
+            testcase, compiler, _ = line.split()
+            best = average = 0
+        averages.setdefault(compiler, dict())[testcase] = int(average)
+        testcases.add(testcase)
+        compilers.add(compiler)
+
+    compilers = sorted(compilers)
+    testcases = sorted(testcases)
+
+    # eventually normalize against Python
+    if normalize:
+        assert normalize in compilers, "normalize against an existing compiler"
+        for testcase in testcases:
+            ref = float(averages[normalize][testcase])
+            for compiler in compilers:
+                average = averages[compiler][testcase]
+                if average:
+                    averages[compiler][testcase] = ref / average
+
+    return averages, testcases, compilers
+
+
+def dump_rst(averages, testcases, compilers):
+    table = [[''] + compilers]
+    for testcase in testcases:
+        times = [averages[compiler][testcase] for compiler in compilers]
+        mtime = min([time for time in times if time] or [0.])
+        ftimes = [(str(time) if time != mtime else '*{}*'.format(time))
+                  for time in times]
+        table.append([testcase] + ftimes)
+
+    cols = zip(*table)
+    col_widths = [max(len(value) for value in col) for col in cols]
+    format = ' '.join(['%%%ds' % width for width in col_widths])
+    for row in table:
+        print(format % tuple(row))
+
+
+def dump_tex(averages, testcases, compilers):
+    table = []
+    for testcase in testcases:
+        times = [averages[compiler][testcase] for compiler in compilers]
+        mtime = min([time for time in times if time] or [0.])
+        ftimes = [(str(time)
+                   if time != mtime
+                   else r'\textbf{{{}}}'.format(time))
+                  for time in times]
+        table.append([testcase.replace('_', r'\_')] + ftimes)
+
+    cols = zip(*table)
+    col_widths = [max(len(value) for value in col) for col in cols]
+    format = ' & '.join(['%%%ds' % width for width in col_widths])
+
+    print(r'\begin{tabular}{|l||' + 'c|' * len(compilers) + '}')
+    print(r'\hline')
+    print(r'&', ' & '.join(compilers), r'\\')
+    print(r'\hline')
+    print(r'\hline')
+    for row in table:
+        print(format % tuple(row), r'\\')
+    print(r'\hline')
+    print(r'\end{tabular}')
+
+
+def dump_plot(averages, testcases, compilers,
+              flavor='png', normalize='Python'):
+    import matplotlib as mpl
+    import matplotlib.pyplot as plt
+    import numpy as np
+
+    colors = 'rgbkymc'
+    assert len(colors) >= len(compilers)
+
+    fig = plt.figure()
+    ax = fig.add_subplot(1, 1, 1)
+
+    ind = np.arange(len(testcases))
+    width = 1. / (2 + len(compilers))
+
+    # fill the bars
+    rects = []
+    for i, compiler in enumerate(compilers):
+        try:
+            bar = ax.bar(ind + i * width,
+                         [averages[compiler][testcase]
+                          for testcase in testcases],
+                         width,
+                         color=colors[i],
+                         log=True,
+                         )
+            rects.append(bar)
+        except ValueError:
+            pass
+
+    # the bar captions
+    ax.set_xticks(ind+width)
+    xtickNames = ax.set_xticklabels(testcases)
+    plt.setp(xtickNames, rotation=45, fontsize=10)
+
+    # the legends
+    ax.legend([rect[0] for rect in rects],
+              compilers,
+              ncol=2,
+              prop={'size': 10}
+              )
+    if normalize:
+        ax.set_ylabel(u"Speedup with respect to " + normalize)
+    else:
+        ax.set_ylabel(u"Average execution time (Âµs)")
+
+    # plt.show()
+    if len(testcases) == 1:
+        basename = testcases[0]
+    else:
+        basename = 'benchmarks'
+    fullname = basename + '.' + flavor
+    plt.tight_layout()
+    plt.savefig(fullname)
+    print(fullname, 'generated')
+
+def format(args):
+    options = {'normalize': args.normalize}
+    averages, testcases, compilers = read_data(args.log, **options)
+
+    if args.split:
+        def dumper(handler, **kwargs):
+            for testcase in testcases:
+                handler({c: {testcase: averages[c][testcase]}
+                         for c in compilers},
+                        [testcase],
+                        compilers,
+                        **kwargs)
+    else:
+        def dumper(handler, **kwargs):
+            handler(averages, testcases, compilers, **kwargs)
+    if args.type == 'rst':
+        dumper(dump_rst)
+    elif args.type == 'tex':
+        dumper(dump_tex)
+    else:
+        dumper(dump_plot, flavor=args.type, normalize=args.normalize)
+
+
+###############################################################################
+# setup command
+
+
+class CustomVenv(venv.EnvBuilder):
+    '''
+    Virtual environment for the installation.
+    Install wheel in the venv for packaging.
+    '''
+    def __init__(self, *args, **kwargs):
+        self.requirements = kwargs.pop('requirements', [])
+        super().__init__(*args, **kwargs)
+
+    def post_setup(self, context):
+        self.python = context.env_exe
+        if not self.requirements:
+            return
+        def pip(*args):
+            subprocess.run([self.python, '-m', 'pip', *args], check=True)
+        pip('install', '-U', 'pip')
+        pip('install', *self.requirements)
+
+def setup(args):
+    builder = CustomVenv(with_pip=True,
+                         requirements=[
+                             'numpy',
+                             'numba',
+                             'pythran',
+                             'hope',
+                             #
+                             'matplotlib',
+                         ])
+    builder.create(args.venv)
+
+###############################################################################
+# main
+
+def main():
+    import glob
+    import argparse
+    import sys
+    parser = argparse.ArgumentParser(prog='numpy-benchmarks',
+                                     description='run synthetic numpy benchmarks',
+                                     epilog="It's a megablast!")
+    parser.add_argument("--venv", default=".npb")
+    parser.add_argument("--in-venv", action='store_true',
+                        help=argparse.SUPPRESS)
+
+    subparser = parser.add_subparsers()
+    clean_parser = subparser.add_parser("clean",
+                                        help="clean previous bench")
+    clean_parser.set_defaults(func=clean)
+
+    setup_parser = subparser.add_parser("setup",
+                                        help="setup bench environment")
+    setup_parser.set_defaults(func=setup)
+
+    format_parser = subparser.add_parser("format",
+                                         help="format the output of a bench")
+    format_parser.add_argument("log", help="log file to format")
+    format_parser.add_argument('-t', '--type', default='rst',
+                        help='output format [{}] (default=rst)'.format(
+                            ", ".join(("rst",
+                                       "tex",
+                                       "eps",
+                                       "pdf",
+                                       "pgf",
+                                       "png",
+                                       "ps",
+                                       "raw",
+                                       "rgba",
+                                       "svg",
+                                       "svgz"))))
+    format_parser.add_argument('--logscale', action='store_true',
+                        help='use logarithmic scale (default=False)')
+    format_parser.add_argument('--normalize', type=str, default="",
+                        help='normalize against given result (default="")')
+    format_parser.add_argument('--split', action='store_true',
+                        help='split output in multiple parts')
+    format_parser.set_defaults(func=format)
+
+    run_parser = subparser.add_parser("run",
+                                      help="run a set of bench")
+    run_parser.add_argument('benchmarks', nargs='*',
+                        help='benchmark to run, default is benchmarks/*',
+                        default=glob.glob('benchmarks/*.py'))
+    run_parser.add_argument('-t', action='append', dest='targets', metavar='TARGET',
+                        help='target compilers to use, default is %s' % ', '.join(default_targets))
+    run_parser.set_defaults(func=run)
+    args = parser.parse_args(sys.argv[1:])
+
+    if args.func is not setup and not args.in_venv:
+        builder = CustomVenv(with_pip=True)
+        builder.create(args.venv)
+        new_args = [builder.python, "./np-bench", "--in-venv"] + sys.argv[1:]
+        os.execl(builder.python, *new_args)
+
+    args.func(args)
+
+###############################################################################
+# benchit script
+
+benchit = """
+
+'''
+An adaptation from timeit that outputs some extra statistical informations
+'''
+
+from timeit import default_timer, default_repeat, Timer
+import numpy
+import sys
+import time
+
+
+def main(args=None):
+    '''Main program, used when run as a script.
+
+    The optional argument specifies the command line to be parsed,
+    defaulting to sys.argv[1:].
+
+    The return value is an exit code to be passed to sys.exit(); it
+    may be None to indicate success.
+
+    When an exception happens during timing, a traceback is printed to
+    stderr and the return value is 1.  Exceptions at other times
+    (including the template compilation) are not caught.
+    '''
+    if not args:
+        args = sys.argv[1:]
+    import getopt
+    try:
+        opts, args = getopt.getopt(args, 'n:s:r:tcvh',
+                                   ['number=', 'setup=', 'repeat=',
+                                    'time', 'clock', 'verbose', 'help'])
+    except getopt.error as err:
+        print(err)
+        print("use -h/--help for command line help")
+        return 2
+    timer = default_timer
+    stmt = "\\n".join(args) or "pass"
+    number = 0  # auto-determine
+    setup = []
+    repeat = default_repeat
+    verbose = 0
+    precision = 3
+    for o, a in opts:
+        if o in ("-n", "--number"):
+            number = int(a)
+        if o in ("-s", "--setup"):
+            setup.append(a)
+        if o in ("-r", "--repeat"):
+            repeat = int(a)
+            if repeat <= 0:
+                repeat = 1
+        if o in ("-t", "--time"):
+            timer = time.time
+        if o in ("-c", "--clock"):
+            timer = time.clock
+        if o in ("-v", "--verbose"):
+            if verbose:
+                precision += 1
+            verbose += 1
+        if o in ("-h", "--help"):
+            print(__doc__)
+            return 0
+    setup = "\\n".join(setup) or "pass"
+    # Include the current directory, so that local imports work (sys.path
+    # contains the directory of this script, rather than the current
+    # directory)
+    import os
+    sys.path.insert(0, os.curdir)
+    t = Timer(stmt, setup, timer)
+    if number == 0:
+        # determine number so that 0.2 <= total time < 2.0
+        for i in range(1, 10):
+            number = 10**i
+            try:
+                x = t.timeit(number)
+            except:
+                t.print_exc()
+                return 1
+            if verbose:
+                print("%d loops -> %.*g secs" % (number, precision, x))
+            if x >= 0.2:
+                break
+    try:
+        r = t.repeat(repeat, number)
+    except:
+        t.print_exc()
+        return 1
+    if verbose:
+        print("raw times:", " ".join(["%.*g" % (precision, x) for x in r]))
+    r = [int(x * 1e6 / number) for x in r]
+    best = min(r)
+    average = int(numpy.average(r))
+    std = int(numpy.std(r))
+
+    print(best, average, std)
+
+
+if __name__ == "__main__":
+    sys.exit(main())
+"""
+
+if __name__ == '__main__':
+    main()
